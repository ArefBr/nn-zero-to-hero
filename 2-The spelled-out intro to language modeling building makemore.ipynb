{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PART1: Manual Allocation"
      ],
      "metadata": {
        "id": "1jTSSmG6g0qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading and exploring the dataset"
      ],
      "metadata": {
        "id": "vJNRegrO-Cto"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "5hkL3BtT_bu_",
        "outputId": "4d3e4339-a1bc-4595-8d93-5c59e3a4eded"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b899c93a5ed4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'names.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'names.txt'"
          ]
        }
      ],
      "source": [
        "words = open('names.txt', 'r').read().splitlines()\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring and counting the bigrams in the dataset"
      ],
      "metadata": {
        "id": "3Ds92oKs-Xzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = {}\n",
        "for w in words:\n",
        "    chs = ['<S>'] + list(w) + ['<E>']\n",
        "    for ch1, ch2 in zip(chs,chs[1:]):\n",
        "        bigram = (ch1,ch2)\n",
        "        b[bigram] = b.get(bigram,0) + 1"
      ],
      "metadata": {
        "id": "PP34CkWe_iwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(b.items(), key = lambda kv:-kv[1])"
      ],
      "metadata": {
        "id": "IYH6WUd-AEmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Counting bigrams in a 2D torch tensor (\"training the model\")"
      ],
      "metadata": {
        "id": "WG6tTAa8-xvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "N = torch.zeros((27,27), dtype=torch.int32)"
      ],
      "metadata": {
        "id": "ZIvUc4kCSHR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a list of eng alphabet\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "Fe_Se9YxSMIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2 in zip(chs,chs[1:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        \n",
        "        N[ix1,ix2] +=1"
      ],
      "metadata": {
        "id": "S-4261WzSNyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the bigram tensor"
      ],
      "metadata": {
        "id": "A2LZg7BE-5Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(N, cmap='Blues')\n",
        "for i in range(27):\n",
        "    for j in range(27):\n",
        "        chstr = itos[i] + itos[j]\n",
        "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
        "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "bUwGPNP1SR6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficiency! vectorized normalization of the rows, tensor broadcasting"
      ],
      "metadata": {
        "id": "WX_FbW8f_Rno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# first row\n",
        "p = N[0,:].float()\n",
        "p = p / p.sum()\n",
        "p"
      ],
      "metadata": {
        "id": "xwRcVUd4SThv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "ix = torch.multinomial(p,num_samples=1, replacement=True, generator=g).item()\n",
        "\n",
        "itos[ix]"
      ],
      "metadata": {
        "id": "xx9GEp2Ortkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Andrej Edition\n",
        "\n",
        "P = (N+1).float() # N+1 -> Model Smothing\n",
        "P /= P.sum(1,keepdim=True)"
      ],
      "metadata": {
        "id": "N-kKQWNr3hSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(P.shape)\n",
        "print(P.sum(1,keepdim=True).shape)\n",
        "# 27, 27\n",
        "# 27, 1"
      ],
      "metadata": {
        "id": "nM5cvLBd3-Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P[0].sum()"
      ],
      "metadata": {
        "id": "xFHwNdqD1wjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling from the model"
      ],
      "metadata": {
        "id": "57Ewm0sq_wlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "for i in range(10):\n",
        "  ix = 0\n",
        "  out = []\n",
        "  while True:\n",
        "    p = P[ix]\n",
        "\n",
        "    ix = torch.multinomial(p,num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[ix])\n",
        "    if ix == 0:\n",
        "      break\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "id": "8JWqHY5_ujId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss function (the negative log likelihood of the data under our model)"
      ],
      "metadata": {
        "id": "aAjFb2_s_3KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_likelihood = 0\n",
        "n = 0\n",
        "\n",
        "for w in words:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2 in zip(chs,chs[1:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        \n",
        "        prob = P[ix1,ix2]\n",
        "        logprob = torch.log(prob)\n",
        "\n",
        "        log_likelihood += logprob\n",
        "        n += 1\n",
        "        # print(f'{ch1}{ch2}: {prob:.4f}-> {logprob:.4f}')\n",
        "\n",
        "nll = -log_likelihood\n",
        "print(f'{nll=}')\n",
        "print(f'{nll/n}')"
      ],
      "metadata": {
        "id": "jYtsej0Qw23J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 2: ANN"
      ],
      "metadata": {
        "id": "QFzJ4lGxhUzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the bigram dataset for the neural net"
      ],
      "metadata": {
        "id": "s9QF5BAsmpt4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #crate the training set of bigrams\n",
        "\n",
        "xs, ys = [], []\n",
        "\n",
        "for w in words[:1]:\n",
        "    chs = ['.'] + list(w) + ['.']\n",
        "    for ch1, ch2 in zip(chs,chs[1:]):\n",
        "        ix1 = stoi[ch1]\n",
        "        ix2 = stoi[ch2]\n",
        "        xs.append(ix1)\n",
        "        ys.append(ix2)\n",
        "\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)\n"
      ],
      "metadata": {
        "id": "ZD_7zA5QBU8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs"
      ],
      "metadata": {
        "id": "IcihZ1j3issE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ys"
      ],
      "metadata": {
        "id": "4vQyYCz2itfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feeding integers into neural nets? one-hot encodings"
      ],
      "metadata": {
        "id": "hSkZnfKbm8sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "xenc = F.one_hot(xs,num_classes=27).float()\n",
        "xenc"
      ],
      "metadata": {
        "id": "sWpf0nkDiuB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(xenc)"
      ],
      "metadata": {
        "id": "n7P-j2MflceL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The \"neural net\": one linear layer of neurons implemented with matrix multiplication"
      ],
      "metadata": {
        "id": "Rmf22SXnnHue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.randn((27,27))\n",
        "(xenc @ W)[3,13]\n",
        "# (5,27) @ (27 ,27) -> (5, 27)"
      ],
      "metadata": {
        "id": "ycbPVtJpmER-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(xenc[3] * W[:,13]).sum()"
      ],
      "metadata": {
        "id": "IeCntZo4q67m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc @ W"
      ],
      "metadata": {
        "id": "q-8IQ7yWwfxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transforming neural net outputs into probabilities: the softmax"
      ],
      "metadata": {
        "id": "KGYJUsAAu9Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = (xenc @ W) #log counts\n",
        "counts = logits.exp() #equivalent N\n",
        "prob = counts /counts.sum(1, keepdims=True)\n",
        "prob "
      ],
      "metadata": {
        "id": "yp9L-fFcrPf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob[0] # -> what should come after '.'"
      ],
      "metadata": {
        "id": "fdEaCL7swPIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary, Preview to next steps, reference to Micrograd"
      ],
      "metadata": {
        "id": "6Q7ci4XYypd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xs"
      ],
      "metadata": {
        "id": "wLL5rBUWykMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ys"
      ],
      "metadata": {
        "id": "P0rORz25zL_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g)"
      ],
      "metadata": {
        "id": "GRWt8IMYzNBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
        "logits = xenc @ W # predict log-counts\n",
        "counts = logits.exp() # (fake) counts, equivalent to N\n",
        "probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
        "# btw: the last 2 lines here are together called a 'softmax'"
      ],
      "metadata": {
        "id": "H1rfLIMnzO42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nlls = torch.zeros(5)\n",
        "for i in range(5):\n",
        "  # i-th bigram:\n",
        "  x = xs[i].item() # input character index\n",
        "  y = ys[i].item() # label character index\n",
        "  print('--------')\n",
        "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
        "  print('input to the neural net:', x)\n",
        "  # print('output probabilities from the neural net:', probs[i])\n",
        "  print('label (actual next character):', y)\n",
        "  p = probs[i, y]\n",
        "  print('probability assigned by the net to the the correct character:', p.item())\n",
        "  logp = torch.log(p)\n",
        "  print('log likelihood:', logp.item())\n",
        "  nll = -logp\n",
        "  print('negative log likelihood:', nll.item())\n",
        "  nlls[i] = nll\n",
        "\n",
        "print('==================')\n",
        "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
      ],
      "metadata": {
        "id": "ysPBqXw6zQdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pAf6Tg6l04sq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}